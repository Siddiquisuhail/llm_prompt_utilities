{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & API connection\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Any, Dict, Iterable, List, Optional\n",
    "import tiktoken\n",
    "import html\n",
    "from html import escape  \n",
    "import difflib\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "try:\n",
    "    from IPython.display import HTML, display\n",
    "    _IN_IPYTHON = True\n",
    "except ImportError:\n",
    "    _IN_IPYTHON = False\n",
    "\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "MODEL = \"gpt-4.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_COLORS = {\n",
    "    '+': (\"#d2f5d6\", \"#22863a\"),   # additions  (green)\n",
    "    '-': (\"#f8d7da\", \"#b31d28\"),   # deletions  (red)\n",
    "    '@': (None,      \"#6f42c1\"),   # hunk header (purple)\n",
    "}\n",
    "\n",
    "def _css(**rules: str) -> str:\n",
    "    \"\"\"Convert kwargs to a CSS string (snake_case ‚Üí kebab-case).\"\"\"\n",
    "    return \";\".join(f\"{k.replace('_', '-')}: {v}\" for k, v in rules.items())\n",
    "\n",
    "def _render(html_str: str) -> None:\n",
    "    \"\"\"Render inside Jupyter if available, else print to stdout.\"\"\"\n",
    "    try:\n",
    "        display  # type: ignore[name-defined]\n",
    "        from IPython.display import HTML  # noqa: WPS433\n",
    "        display(HTML(html_str))\n",
    "    except NameError:\n",
    "        print(html_str, flush=True)\n",
    "\n",
    "# ---------- diff helpers ------------------------------------------------------\n",
    "\n",
    "def _style(line: str) -> str:\n",
    "    \"\"\"Wrap a diff line in a <span> with optional colors.\"\"\"\n",
    "    bg, fg = _COLORS.get(line[:1], (None, None))\n",
    "    css = \";\".join(s for s in (f\"background:{bg}\" if bg else \"\",\n",
    "                               f\"color:{fg}\" if fg else \"\") if s)\n",
    "    return f'<span style=\"{css}\">{html.escape(line)}</span>'\n",
    "\n",
    "def _wrap(lines: Iterable[str]) -> str:\n",
    "    body = \"<br>\".join(lines)\n",
    "    return (\n",
    "        \"<details>\"\n",
    "        \"<summary>üïµÔ∏è‚Äç‚ôÇÔ∏è Critique & Diff (click to expand)</summary>\"\n",
    "        f'<div style=\"font-family:monospace;white-space:pre;\">{body}</div>'\n",
    "        \"</details>\"\n",
    "    )\n",
    "\n",
    "def show_critique_and_diff(old: str, new: str) -> str:\n",
    "    \"\"\"Display & return a GitHub-style HTML diff between *old* and *new*.\"\"\"\n",
    "    diff = difflib.unified_diff(old.splitlines(), new.splitlines(),\n",
    "                                fromfile=\"old\", tofile=\"new\", lineterm=\"\")\n",
    "    html_block = _wrap(map(_style, diff))\n",
    "    _render(html_block)\n",
    "    return html_block\n",
    "\n",
    "# ---------- ‚Äúcard‚Äù helpers ----------------------------------------------------\n",
    "\n",
    "CARD    = _css(background=\"#f8f9fa\", border_radius=\"8px\", padding=\"18px 22px\",\n",
    "               margin_bottom=\"18px\", border=\"1px solid #e0e0e0\",\n",
    "               box_shadow=\"0 1px 4px #0001\")\n",
    "TITLE   = _css(font_weight=\"600\", font_size=\"1.1em\", color=\"#2d3748\",\n",
    "               margin_bottom=\"6px\")\n",
    "LABEL   = _css(color=\"#718096\", font_size=\"0.95em\", font_weight=\"500\",\n",
    "               margin_right=\"6px\")\n",
    "EXTRACT = _css(font_family=\"monospace\", background=\"#f1f5f9\", padding=\"7px 10px\",\n",
    "               border_radius=\"5px\", display=\"block\", margin_top=\"3px\",\n",
    "               white_space=\"pre-wrap\", color=\"#1a202c\")\n",
    "\n",
    "def display_cards(\n",
    "    items: Iterable[Any],\n",
    "    *,\n",
    "    title_attr: str,\n",
    "    field_labels: Optional[Dict[str, str]] = None,\n",
    "    card_title_prefix: str = \"Item\",\n",
    ") -> None:\n",
    "    \"\"\"Render objects as HTML ‚Äúcards‚Äù (or plaintext when not in IPython).\"\"\"\n",
    "    items = list(items)\n",
    "    if not items:\n",
    "        _render(\"<em>No data to display.</em>\")\n",
    "        return\n",
    "\n",
    "    # auto-derive field labels if none supplied\n",
    "    if field_labels is None:\n",
    "        sample = items[0]\n",
    "        field_labels = {\n",
    "            a: a.replace(\"_\", \" \").title()\n",
    "            for a in dir(sample)\n",
    "            if not a.startswith(\"_\")\n",
    "            and not callable(getattr(sample, a))\n",
    "            and a != title_attr\n",
    "        }\n",
    "\n",
    "    cards = []\n",
    "    for idx, obj in enumerate(items, 1):\n",
    "        title_html = html.escape(str(getattr(obj, title_attr, \"<missing title>\")))\n",
    "        rows = [f'<div style=\"{TITLE}\">{card_title_prefix} {idx}: {title_html}</div>']\n",
    "\n",
    "        for attr, label in field_labels.items():\n",
    "            value = getattr(obj, attr, None)\n",
    "            if value is None:\n",
    "                continue\n",
    "            rows.append(\n",
    "                f'<div><span style=\"{LABEL}\">{html.escape(label)}:</span>'\n",
    "                f'<span style=\"{EXTRACT}\">{html.escape(str(value))}</span></div>'\n",
    "            )\n",
    "\n",
    "        cards.append(f'<div style=\"{CARD}\">{\"\".join(rows)}</div>')\n",
    "\n",
    "    _render(\"\\n\".join(cards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original prompt length: 244 tokens\n"
     ]
    }
   ],
   "source": [
    "original_prompt = \"\"\"\n",
    "[System]\n",
    "Please act as an impartial judge and evaluate the quality of the responses provided by two\n",
    "AI assistants to the user question displayed below. You should choose the assistant that\n",
    "follows the user‚Äôs instructions and answers the user‚Äôs question better. Your evaluation\n",
    "should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,\n",
    "and level of detail of their responses. Begin your evaluation by comparing the two\n",
    "responses and provide a short explanation. Avoid any position biases and ensure that the\n",
    "order in which the responses were presented does not influence your decision. Do not allow\n",
    "the length of the responses to influence your evaluation. Do not favor certain names of\n",
    "the assistants. Be as objective as possible. After providing your explanation, output your\n",
    "final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\"\n",
    "if assistant B is better, and \"[[C]]\" for a tie.\n",
    "\n",
    "[User Question]\n",
    "{question}\n",
    "\n",
    "[The Start of Assistant A‚Äôs Answer]\n",
    "{answer_a}\n",
    "[The End of Assistant A‚Äôs Answer]\n",
    "\n",
    "[The Start of Assistant B‚Äôs Answer]\n",
    "{answer_b}\n",
    "[The End of Assistant B‚Äôs Answer]\n",
    "\"\"\"\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "num_tokens = len(encoding.encode(original_prompt))\n",
    "print(\"Original prompt length:\", num_tokens, \"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instruction(BaseModel):\n",
    "    instruction_title: str = Field(description=\"A 2-8 word title of the instruction that the LLM has to follow.\")\n",
    "    extracted_instruction: str = Field(description=\"The exact text that was extracted from the system prompt that the instruction is derived from.\")\n",
    "\n",
    "class InstructionList(BaseModel):\n",
    "    instructions: list[Instruction] = Field(description=\"A list of instructions and their corresponding extracted text that the LLM has to follow.\")\n",
    "\n",
    "\n",
    "EXTRACT_INSTRUCTIONS_SYSTEM_PROMPT = \"\"\"\n",
    "## Role & Objective\n",
    "You are an **Instruction-Extraction Assistant**.  \n",
    "Your job is to read a System Prompt provided by the user and distill the **mandatory instructions** the target LLM must obey.\n",
    "\n",
    "## Instructions\n",
    "1. **Identify Mandatory Instructions**  \n",
    "   ‚Ä¢ Locate every instruction in the System Prompt that the LLM is explicitly required to follow.  \n",
    "   ‚Ä¢ Ignore suggestions, best-practice tips, or optional guidance.\n",
    "\n",
    "2. **Generate Rules**  \n",
    "   ‚Ä¢ Re-express each mandatory instruction as a clear, concise rule.\n",
    "   ‚Ä¢ Provide the extracted text that the instruction is derived from.\n",
    "   ‚Ä¢ Each rule must be standalone and imperative.\n",
    "\n",
    "## Output Format\n",
    "Return a json object with a list of instructions which contains an instruction_title and their corresponding extracted text that the LLM has to follow. Do not include any other text or comments.\n",
    "\n",
    "## Constraints\n",
    "- Include **only** rules that the System Prompt explicitly enforces.  \n",
    "- Omit any guidance that is merely encouraged, implied, or optional.  \n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=MODEL,\n",
    "    input=\"SYSTEM_PROMPT TO ANALYZE: \" + original_prompt,\n",
    "    instructions=EXTRACT_INSTRUCTIONS_SYSTEM_PROMPT,\n",
    "    temperature=0.0,\n",
    "    text_format=InstructionList,\n",
    ")\n",
    "\n",
    "instructions_list = response.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background: #f8f9fa;border-radius: 8px;padding: 18px 22px;margin-bottom: 18px;border: 1px solid #e0e0e0;box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600;font-size: 1.1em;color: #2d3748;margin-bottom: 6px\">Instruction 1: Act as impartial judge</div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below.</span></div></div>\n",
       "<div style=\"background: #f8f9fa;border-radius: 8px;padding: 18px 22px;margin-bottom: 18px;border: 1px solid #e0e0e0;box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600;font-size: 1.1em;color: #2d3748;margin-bottom: 6px\">Instruction 2: Choose better assistant</div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">You should choose the assistant that follows the user‚Äôs instructions and answers the user‚Äôs question better.</span></div></div>\n",
       "<div style=\"background: #f8f9fa;border-radius: 8px;padding: 18px 22px;margin-bottom: 18px;border: 1px solid #e0e0e0;box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600;font-size: 1.1em;color: #2d3748;margin-bottom: 6px\">Instruction 3: Consider specific evaluation factors</div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">Your evaluation should consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses.</span></div></div>\n",
       "<div style=\"background: #f8f9fa;border-radius: 8px;padding: 18px 22px;margin-bottom: 18px;border: 1px solid #e0e0e0;box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600;font-size: 1.1em;color: #2d3748;margin-bottom: 6px\">Instruction 4: Begin with comparison and explanation</div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">Begin your evaluation by comparing the two responses and provide a short explanation.</span></div></div>\n",
       "<div style=\"background: #f8f9fa;border-radius: 8px;padding: 18px 22px;margin-bottom: 18px;border: 1px solid #e0e0e0;box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600;font-size: 1.1em;color: #2d3748;margin-bottom: 6px\">Instruction 5: Avoid position bias</div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision.</span></div></div>\n",
       "<div style=\"background: #f8f9fa;border-radius: 8px;padding: 18px 22px;margin-bottom: 18px;border: 1px solid #e0e0e0;box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600;font-size: 1.1em;color: #2d3748;margin-bottom: 6px\">Instruction 6: Do not let response length influence evaluation</div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">Do not allow the length of the responses to influence your evaluation.</span></div></div>\n",
       "<div style=\"background: #f8f9fa;border-radius: 8px;padding: 18px 22px;margin-bottom: 18px;border: 1px solid #e0e0e0;box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600;font-size: 1.1em;color: #2d3748;margin-bottom: 6px\">Instruction 7: Do not favor assistant names</div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">Do not favor certain names of the assistants.</span></div></div>\n",
       "<div style=\"background: #f8f9fa;border-radius: 8px;padding: 18px 22px;margin-bottom: 18px;border: 1px solid #e0e0e0;box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600;font-size: 1.1em;color: #2d3748;margin-bottom: 6px\">Instruction 8: Be objective</div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">Be as objective as possible.</span></div></div>\n",
       "<div style=\"background: #f8f9fa;border-radius: 8px;padding: 18px 22px;margin-bottom: 18px;border: 1px solid #e0e0e0;box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600;font-size: 1.1em;color: #2d3748;margin-bottom: 6px\">Instruction 9: Output final verdict in strict format</div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Extracted Text:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">After providing your explanation, output your final verdict by strictly following this format: &quot;[[A]]&quot; if assistant A is better, &quot;[[B]]&quot; if assistant B is better, and &quot;[[C]]&quot; for a tie.</span></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_cards(\n",
    "    instructions_list.instructions,\n",
    "    title_attr=\"instruction_title\",\n",
    "    field_labels={\"extracted_instruction\": \"Extracted Text\"},\n",
    "    card_title_prefix=\"Instruction\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CritiqueIssue(BaseModel):\n",
    "    issue: str\n",
    "    snippet: str\n",
    "    explanation: str\n",
    "    suggestion: str\n",
    "\n",
    "class CritiqueIssues(BaseModel):\n",
    "    issues: List[CritiqueIssue] = Field(..., min_length=1, max_length=6)\n",
    "    \n",
    "CRITIQUE_SYSTEM_PROMPT = \"\"\"\n",
    "## Role & Objective  \n",
    "You are a **Prompt-Critique Assistant**.\n",
    "Examine a user-supplied LLM prompt (targeting GPT-4.1 or compatible) and surface any weaknesses.\n",
    "\n",
    "## Instructions\n",
    "Check for the following issues:\n",
    "- Ambiguity: Could any wording be interpreted in more than one way?\n",
    "- Lacking Definitions: Are there any class labels, terms, or concepts that are not defined that might be misinterpreted by an LLM?\n",
    "- Conflicting, missing, or vague instructions: Are directions incomplete or contradictory?\n",
    "- Unstated assumptions: Does the prompt assume the model has to be able to do something that is not explicitly stated?\n",
    "\n",
    "## Do **NOT** list issues of the following types:\n",
    "- Invent new instructions, tool calls, or external information. You do not know what tools need to be added that are missing.\n",
    "- Issues that you are not sure about.\n",
    "\n",
    "## Output Format  \n",
    "Return a JSON **array** (not an object) with 1-6 items, each following this schema:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"issue\":      \"<1-6 word label>\",\n",
    "  \"snippet\":    \"<‚â§50-word excerpt>\",\n",
    "  \"explanation\":\"<Why it matters>\",\n",
    "  \"suggestion\": \"<Actionable fix>\"\n",
    "}\n",
    "Return a JSON array of these objects. If the prompt is already clear, complete, and effective, return an empty list: `[]`.\n",
    "\"\"\"\n",
    "\n",
    "CRITIQUE_USER_PROMPT = f\"\"\"\n",
    "Evaluate the following prompt for clarity, completeness, and effectiveness:\n",
    "###\n",
    "{original_prompt}\n",
    "###\n",
    "Return your critique using the specified JSON format only.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.parse(\n",
    "    model=MODEL,\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": CRITIQUE_SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": CRITIQUE_USER_PROMPT},\n",
    "    ],\n",
    "    temperature=0.0,\n",
    "    text_format=CritiqueIssues,\n",
    ")\n",
    "\n",
    "critique = response.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background: #f8f9fa;border-radius: 8px;padding: 18px 22px;margin-bottom: 18px;border: 1px solid #e0e0e0;box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600;font-size: 1.1em;color: #2d3748;margin-bottom: 6px\">Issue 1: Ambiguity in &#x27;short explanation&#x27;</div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Snippet:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">provide a short explanation</span></div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Explanation:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">The prompt does not specify what constitutes a &#x27;short&#x27; explanation, which could lead to inconsistent response lengths or insufficient detail.</span></div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Suggestion:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">Specify a word or sentence limit, e.g., &#x27;Provide a 2-3 sentence explanation.&#x27;</span></div></div>\n",
       "<div style=\"background: #f8f9fa;border-radius: 8px;padding: 18px 22px;margin-bottom: 18px;border: 1px solid #e0e0e0;box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600;font-size: 1.1em;color: #2d3748;margin-bottom: 6px\">Issue 2: Unclear weighting of evaluation factors</div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Snippet:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail</span></div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Explanation:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">The prompt lists several evaluation factors but does not clarify if they should be weighted equally or if some are more important than others.</span></div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Suggestion:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">Clarify if all factors are equally important or if some should be prioritized.</span></div></div>\n",
       "<div style=\"background: #f8f9fa;border-radius: 8px;padding: 18px 22px;margin-bottom: 18px;border: 1px solid #e0e0e0;box-shadow: 0 1px 4px #0001\"><div style=\"font-weight: 600;font-size: 1.1em;color: #2d3748;margin-bottom: 6px\">Issue 3: Potential ambiguity in &#x27;answers the user‚Äôs question better&#x27;</div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Snippet:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">answers the user‚Äôs question better</span></div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Explanation:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">The phrase could be interpreted in multiple ways, such as focusing solely on directness or on overall quality, leading to inconsistent judgments.</span></div><div><span style=\"color: #718096;font-size: 0.95em;font-weight: 500;margin-right: 6px\">Suggestion:</span><span style=\"font-family: monospace;background: #f1f5f9;padding: 7px 10px;border-radius: 5px;display: block;margin-top: 3px;white-space: pre-wrap;color: #1a202c\">Define what &#x27;answers better&#x27; means in this context, e.g., &#x27;provides the most accurate, complete, and user-aligned response.&#x27;</span></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_cards(\n",
    "    critique.issues,\n",
    "    title_attr=\"issue\",\n",
    "    field_labels={\n",
    "        \"snippet\": \"Snippet\",\n",
    "        \"explanation\": \"Explanation\",\n",
    "        \"suggestion\": \"Suggestion\"\n",
    "    },\n",
    "    card_title_prefix=\"Issue\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue: Ambiguity in 'short explanation'\n",
      "Snippet: provide a short explanation\n",
      "Explanation: The prompt does not specify what constitutes a 'short' explanation, which could lead to inconsistent response lengths or insufficient detail.\n",
      "Suggestion: Specify a word or sentence limit, e.g., 'Provide a 2-3 sentence explanation.'\n",
      "\n",
      "Issue: Unclear weighting of evaluation factors\n",
      "Snippet: consider factors such as the helpfulness, relevance, accuracy, depth, creativity, and level of detail\n",
      "Explanation: The prompt lists several evaluation factors but does not clarify if they should be weighted equally or if some are more important than others.\n",
      "Suggestion: Clarify if all factors are equally important or if some should be prioritized.\n",
      "\n",
      "Issue: Potential ambiguity in 'answers the user‚Äôs question better'\n",
      "Snippet: answers the user‚Äôs question better\n",
      "Explanation: The phrase could be interpreted in multiple ways, such as focusing solely on directness or on overall quality, leading to inconsistent judgments.\n",
      "Suggestion: Define what 'answers better' means in this context, e.g., 'provides the most accurate, complete, and user-aligned response.'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a string of the issues\n",
    "issues_str = \"\\n\".join(\n",
    "    f\"Issue: {issue.issue}\\nSnippet: {issue.snippet}\\nExplanation: {issue.explanation}\\nSuggestion: {issue.suggestion}\\n\"\n",
    "    for issue in critique.issues\n",
    ")\n",
    "\n",
    "print(issues_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "REVISE_SYSTEM_PROMPT = \"\"\"\n",
    "## Role & Objective  \n",
    "Revise the user‚Äôs original prompt to resolve most of the listed issues, while preserving the original wording and structure as much as possible.\n",
    "\n",
    "## Instructions\n",
    "1. Carefully review the original prompt and the list of issues.\n",
    "2. Apply targeted edits directly addressing the listed issues. The edits should be as minimal as possible while still addressing the issue.\n",
    "3. Do not introduce new content or make assumptions beyond the provided information.\n",
    "4. Maintain the original structure and format of the prompt.\n",
    "\n",
    "## Output Format\n",
    "Return only the fully revised prompt. Do not include commentary, summaries, or code fences.\n",
    "\"\"\"\n",
    "\n",
    "REVISE_USER_PROMPT = f\"\"\"\n",
    "Here is the original prompt:\n",
    "---\n",
    "{original_prompt}\n",
    "---\n",
    "\n",
    "Here are the issues to fix:\n",
    "{issues_str}\n",
    "\n",
    "Please return **only** the fully revised prompt. Do not include commentary, summaries, or explanations.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Revised prompt:\n",
      "------------------\n",
      "[System]\n",
      "Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user‚Äôs instructions and provides the most accurate, complete, and user-aligned response. Your evaluation should consider the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses, weighing all these factors equally. Begin your evaluation by comparing the two responses and provide a 2-3 sentence explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: \"[[A]]\" if assistant A is better, \"[[B]]\" if assistant B is better, and \"[[C]]\" for a tie.\n",
      "\n",
      "[User Question]\n",
      "{question}\n",
      "\n",
      "[The Start of Assistant A‚Äôs Answer]\n",
      "{answer_a}\n",
      "[The End of Assistant A‚Äôs Answer]\n",
      "\n",
      "[The Start of Assistant B‚Äôs Answer]\n",
      "{answer_b}\n",
      "[The End of Assistant B‚Äôs Answer]\n"
     ]
    }
   ],
   "source": [
    "revised_response = client.responses.create(\n",
    "    model=MODEL,\n",
    "    input=REVISE_USER_PROMPT,\n",
    "    instructions=REVISE_SYSTEM_PROMPT,\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "revised_prompt = revised_response.output_text\n",
    "print(\"\\nüîÑ Revised prompt:\\n------------------\")\n",
    "print(revised_response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<details><summary>üïµÔ∏è‚Äç‚ôÇÔ∏è Critique & Diff (click to expand)</summary><div style=\"font-family:monospace;white-space:pre;\"><span style=\"background:#f8d7da;color:#b31d28\">--- old</span><br><span style=\"background:#d2f5d6;color:#22863a\">+++ new</span><br><span style=\"color:#6f42c1\">@@ -1,16 +1,5 @@</span><br><span style=\"background:#f8d7da;color:#b31d28\">-</span><br><span style=\"\"> [System]</span><br><span style=\"background:#f8d7da;color:#b31d28\">-Please act as an impartial judge and evaluate the quality of the responses provided by two</span><br><span style=\"background:#f8d7da;color:#b31d28\">-AI assistants to the user question displayed below. You should choose the assistant that</span><br><span style=\"background:#f8d7da;color:#b31d28\">-follows the user‚Äôs instructions and answers the user‚Äôs question better. Your evaluation</span><br><span style=\"background:#f8d7da;color:#b31d28\">-should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,</span><br><span style=\"background:#f8d7da;color:#b31d28\">-and level of detail of their responses. Begin your evaluation by comparing the two</span><br><span style=\"background:#f8d7da;color:#b31d28\">-responses and provide a short explanation. Avoid any position biases and ensure that the</span><br><span style=\"background:#f8d7da;color:#b31d28\">-order in which the responses were presented does not influence your decision. Do not allow</span><br><span style=\"background:#f8d7da;color:#b31d28\">-the length of the responses to influence your evaluation. Do not favor certain names of</span><br><span style=\"background:#f8d7da;color:#b31d28\">-the assistants. Be as objective as possible. After providing your explanation, output your</span><br><span style=\"background:#f8d7da;color:#b31d28\">-final verdict by strictly following this format: &quot;[[A]]&quot; if assistant A is better, &quot;[[B]]&quot;</span><br><span style=\"background:#f8d7da;color:#b31d28\">-if assistant B is better, and &quot;[[C]]&quot; for a tie.</span><br><span style=\"background:#d2f5d6;color:#22863a\">+Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user‚Äôs instructions and provides the most accurate, complete, and user-aligned response. Your evaluation should consider the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses, weighing all these factors equally. Begin your evaluation by comparing the two responses and provide a 2-3 sentence explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: &quot;[[A]]&quot; if assistant A is better, &quot;[[B]]&quot; if assistant B is better, and &quot;[[C]]&quot; for a tie.</span><br><span style=\"\"> </span><br><span style=\"\"> [User Question]</span><br><span style=\"\"> {question}</span></div></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'<details><summary>üïµÔ∏è\\u200d‚ôÇÔ∏è Critique & Diff (click to expand)</summary><div style=\"font-family:monospace;white-space:pre;\"><span style=\"background:#f8d7da;color:#b31d28\">--- old</span><br><span style=\"background:#d2f5d6;color:#22863a\">+++ new</span><br><span style=\"color:#6f42c1\">@@ -1,16 +1,5 @@</span><br><span style=\"background:#f8d7da;color:#b31d28\">-</span><br><span style=\"\"> [System]</span><br><span style=\"background:#f8d7da;color:#b31d28\">-Please act as an impartial judge and evaluate the quality of the responses provided by two</span><br><span style=\"background:#f8d7da;color:#b31d28\">-AI assistants to the user question displayed below. You should choose the assistant that</span><br><span style=\"background:#f8d7da;color:#b31d28\">-follows the user‚Äôs instructions and answers the user‚Äôs question better. Your evaluation</span><br><span style=\"background:#f8d7da;color:#b31d28\">-should consider factors such as the helpfulness, relevance, accuracy, depth, creativity,</span><br><span style=\"background:#f8d7da;color:#b31d28\">-and level of detail of their responses. Begin your evaluation by comparing the two</span><br><span style=\"background:#f8d7da;color:#b31d28\">-responses and provide a short explanation. Avoid any position biases and ensure that the</span><br><span style=\"background:#f8d7da;color:#b31d28\">-order in which the responses were presented does not influence your decision. Do not allow</span><br><span style=\"background:#f8d7da;color:#b31d28\">-the length of the responses to influence your evaluation. Do not favor certain names of</span><br><span style=\"background:#f8d7da;color:#b31d28\">-the assistants. Be as objective as possible. After providing your explanation, output your</span><br><span style=\"background:#f8d7da;color:#b31d28\">-final verdict by strictly following this format: &quot;[[A]]&quot; if assistant A is better, &quot;[[B]]&quot;</span><br><span style=\"background:#f8d7da;color:#b31d28\">-if assistant B is better, and &quot;[[C]]&quot; for a tie.</span><br><span style=\"background:#d2f5d6;color:#22863a\">+Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question displayed below. You should choose the assistant that follows the user‚Äôs instructions and provides the most accurate, complete, and user-aligned response. Your evaluation should consider the helpfulness, relevance, accuracy, depth, creativity, and level of detail of their responses, weighing all these factors equally. Begin your evaluation by comparing the two responses and provide a 2-3 sentence explanation. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the responses to influence your evaluation. Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, output your final verdict by strictly following this format: &quot;[[A]]&quot; if assistant A is better, &quot;[[B]]&quot; if assistant B is better, and &quot;[[C]]&quot; for a tie.</span><br><span style=\"\"> </span><br><span style=\"\"> [User Question]</span><br><span style=\"\"> {question}</span></div></details>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_critique_and_diff(original_prompt, revised_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Best Practicies of GPT 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEST_PRACTICES_SYSTEM_PROMPT = \"\"\"\n",
    "## Task\n",
    "Your task is to take a **Baseline Prompt** (provided by the user) and output a **Revised Prompt** that keeps the original wording and order as intact as possible **while surgically inserting improvements that follow the ‚ÄúGPT‚Äë4.1‚ÄØBest‚ÄØPractices‚Äù reference**.\n",
    "\n",
    "## How to Edit\n",
    "1. **Keep original text** ‚Äî Only remove something if it directly goes against a best practice. Otherwise, keep the wording, order, and examples as they are.\n",
    "2. **Add best practices only when clearly helpful.** If a guideline doesn‚Äôt fit the prompt or its use case (e.g., diff‚Äëformat guidance on a non‚Äëcoding prompt), just leave that part of the prompt unchanged.\n",
    "3. **Where to add improvements** (use Markdown `#` headings):\n",
    "   - At the very top, add *Agentic Reminders* (like Persistence, Tool-calling, or Planning) ‚Äî only if relevant. Don‚Äôt add these if the prompt doesn‚Äôt require agentic behavior (agentic means prompts that involve planning or running tools for a while).\n",
    "   - When adding sections, follow this order if possible. If some sections do not make sense, don't add them:\n",
    "     1. `# Role & Objective`  \n",
    "        - State who the model is supposed to be (the role) and what its main goal is.\n",
    "     2. `# Instructions`  \n",
    "        - List the steps, rules, or actions the model should follow to complete the task.\n",
    "     3. *(Any sub-sections)*  \n",
    "        - Include any extra sections such as sub-instructions, notes or guidelines already in the prompt that don‚Äôt fit into the main categories.\n",
    "     4. `# Reasoning Steps`  \n",
    "        - Explain the step-by-step thinking or logic the model should use when working through the task.\n",
    "     5. `# Output Format`  \n",
    "        - Describe exactly how the answer should be structured or formatted (e.g., what sections to include, how to label things, or what style to use).\n",
    "     6. `# Examples`  \n",
    "        - Provide sample questions and answers or sample outputs to show the model what a good response looks like.\n",
    "     7. `# Context`  \n",
    "        - Supply any background information, retrieved context, or extra details that help the model understand the task better.\n",
    "   - Don‚Äôt introduce new sections that don‚Äôt exist in the Baseline Prompt. For example, if there‚Äôs no `# Examples` or no `# Context` section, don‚Äôt add one.\n",
    "4. If the prompt is for long context analysis or long tool use, repeat key Agentic Reminders, Important Reminders and Output Format points at the end.\n",
    "5. If there are class labels, evaluation criterias or key concepts, add a definition to each to define them concretely.\n",
    "5. Add a chain-of-thought trigger at the end of main instructions (like ‚ÄúThink step by step...‚Äù), unless one is already there or it would be repetitive.\n",
    "6. For prompts involving tools or sample phrases, add Failure-mode bullets:\n",
    "   - ‚ÄúIf you don‚Äôt have enough info to use a tool, ask the user first.‚Äù\n",
    "   - ‚ÄúVary sample phrases to avoid repetition.‚Äù\n",
    "7. Match the original tone (formal or casual) in anything you add.\n",
    "8. **Only output the full Revised Prompt** ‚Äî no explanations, comments, or diffs. Do not output \"keep the original...\", you need to fully output the prompt, no shortcuts.\n",
    "9. Do not delete any sections or parts that are useful and add value to the prompt and doesn't go against the best practices.\n",
    "10. **Self-check before sending:** Make sure there are no typos, duplicated lines, missing headings, or missed steps.\n",
    "\n",
    "\n",
    "## GPT‚Äë4.1 Best Practices Reference  \n",
    "1. **Persistence reminder**: Explicitly instructs the model to continue working until the user's request is fully resolved, ensuring the model does not stop early.\n",
    "2. **Tool‚Äëcalling reminder**: Clearly tells the model to use available tools or functions instead of making assumptions or guesses, which reduces hallucinations.\n",
    "3. **Planning reminder**: Directs the model to create a step‚Äëby‚Äëstep plan and reflect before and after tool calls, leading to more accurate and thoughtful output.\n",
    "4. **Scaffold structure**: Requires a consistent and predictable heading order (e.g., Role, Instructions, Output‚ÄØFormat) to make prompts easier to maintain.\n",
    "5. **Instruction placement (long context)**: Ensures that key instructions are duplicated or placed strategically so they remain visible and effective in very long prompts.\n",
    "6. **Chain‚Äëof‚Äëthought trigger**: Adds a phrase that encourages the model to reason step by step, which improves logical and thorough responses.\n",
    "7. **Instruction‚Äëconflict hygiene**: Checks for and removes any contradictory instructions, ensuring that the most recent or relevant rule takes precedence.\n",
    "8. **Failure‚Äëmode mitigations**: Adds safeguards against common errors, such as making empty tool calls or repeating phrases, to improve reliability.\n",
    "9. **Diff‚ÄØ/‚ÄØcode‚Äëedit format**: Specifies a robust, line‚Äënumber‚Äëfree diff or code‚Äëedit style for output, making changes clear and easy to apply.\n",
    "10. **Label Definitions**: Defines all the key labels or terms that are used in the prompt so that the model knows what they mean.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'Your organization must be verified to use the model `o3`. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFoundError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m best_practices_response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mo3\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mBASELINE_PROMPT: \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevised_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBEST_PRACTICES_SYSTEM_PROMPT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meffort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhigh\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m improved_prompt = best_practices_response.output_text\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mImproved prompt:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Utilities\\env\\Lib\\site-packages\\openai\\resources\\responses\\responses.py:735\u001b[39m, in \u001b[36mResponses.create\u001b[39m\u001b[34m(self, background, include, input, instructions, max_output_tokens, max_tool_calls, metadata, model, parallel_tool_calls, previous_response_id, prompt, reasoning, service_tier, store, stream, temperature, text, tool_choice, tools, top_logprobs, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    702\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    703\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    704\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    733\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    734\u001b[39m ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/responses\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackground\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackground\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    742\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    743\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_output_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    744\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    745\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    746\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    747\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    748\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_response_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    749\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    755\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    757\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    758\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    760\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    761\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    762\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    763\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsStreaming\u001b[49m\n\u001b[32m    764\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    765\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    766\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    772\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Utilities\\env\\Lib\\site-packages\\openai\\_base_client.py:1256\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1244\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1251\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1252\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1253\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1254\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1255\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\AI_Utilities\\env\\Lib\\site-packages\\openai\\_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNotFoundError\u001b[39m: Error code: 404 - {'error': {'message': 'Your organization must be verified to use the model `o3`. Please go to: https://platform.openai.com/settings/organization/general and click on Verify Organization. If you just verified, it can take up to 15 minutes for access to propagate.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "best_practices_response = client.responses.create(\n",
    "    model=\"o3\",\n",
    "    input=\"BASELINE_PROMPT: \" + revised_prompt,\n",
    "    instructions=BEST_PRACTICES_SYSTEM_PROMPT,\n",
    "    reasoning={\"effort\": \"high\"}\n",
    ")\n",
    "\n",
    "improved_prompt = best_practices_response.output_text\n",
    "print(\"\\nImproved prompt:\\n\")\n",
    "print(improved_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'improved_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m show_critique_and_diff(revised_prompt, \u001b[43mimproved_prompt\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'improved_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "show_critique_and_diff(revised_prompt, improved_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
